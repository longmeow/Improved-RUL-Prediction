{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2ec22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.normalization import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117bd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"1-D Convolution layer to extract high-level features of each time-series input\n",
    "    :param n_features: Number of input features/nodes (d_model)\n",
    "    :param window_size: length of the input sequence\n",
    "    :param kernel_size: size of kernel to use in the convolution operation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, kernel_size):\n",
    "        super().__init__()\n",
    "        self.padding = nn.ConstantPad1d((kernel_size - 1) // 2, 0.0)\n",
    "        self.conv = nn.Conv1d(in_channels=n_features, out_channels=n_features, kernel_size=kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    " \n",
    "    def forward(self, src):\n",
    "        src = src.permute(0, 2, 1)\n",
    "        src = self.padding(src)\n",
    "        src = self.relu(self.conv(src))\n",
    "        return src.permute(0, 2, 1)  # Permute back\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, linear, conv):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed\n",
    "        self.linear = linear\n",
    "        self.conv = conv\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.conv(src)\n",
    "        output = F.relu(self.linear(\n",
    "            self.encoder(self.src_embed(src), src_mask)))\n",
    "        return output\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    Produce N identical layers\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Apply residual connection to any sublayer with the same size.\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder is made up of self-attn and feed foward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "def attention(query, key, value, device, mask=None, dropout=0.0):\n",
    "    \"\"\"Compute the Scaled Dot-Product Attention\"\"\"\n",
    "    query = query.to(device)\n",
    "    key = query.to(device)\n",
    "    value = query.to(device)\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.to(device)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    p_attn = F.dropout(p_attn, p=dropout)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, device, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Takes in model size and number of heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.p = dropout\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        x, self.attn = attention(query, key, value, self.device, mask=mask, dropout=self.p)\n",
    "        # 3) \"Concat\" using a view and apply a final linear\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Torch linears have a \"b\" by default.\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Implements the PE function. \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        try:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        except:\n",
    "            div_term = torch.exp(torch.arange(0, d_model - 1, 2)\n",
    "                                 * (-math.log(10000.0) / d_model))\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1), :], requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class FNetHybridModel(nn.Module):\n",
    "    def __init__(self, trans, src_embed, linear, fnet, conv):\n",
    "        super().__init__()\n",
    "        self.trans = trans\n",
    "        self.src_embed = src_embed\n",
    "        self.linear = linear\n",
    "        self.fnet = fnet\n",
    "        self.conv = conv \n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.conv(src)\n",
    "        output = F.relu(self.linear(self.fnet(\n",
    "            self.trans(self.src_embed(src), src_mask))))\n",
    "        return output\n",
    "\n",
    "\n",
    "class FNetEncoder(nn.Module):\n",
    "    \"Core Fnet is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Pass the input through each layer in turn\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "\n",
    "class FNetEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder is made up of a Fourier Mixing Layer and a FF Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, fft, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.fft = fft\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sublayer[0](x, lambda x: self.fft(x, x, x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class FourierFFTLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        assert query is key\n",
    "        assert key is value\n",
    "\n",
    "        x = query\n",
    "        return torch.real(torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2))\n",
    "\n",
    "\n",
    "def create_transformer(N , d_model, l_win, device, kernel_size, d_ff=0, h=8, dropout=0.1):\n",
    "    if (d_ff == 0):\n",
    "        d_ff = d_model * 4\n",
    "    c = copy.deepcopy\n",
    "    conv = ConvLayer(d_model, kernel_size)\n",
    "    attn = MultiHeadAttention(h, d_model, device, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout, l_win)\n",
    "    final_linear = nn.Sequential(\n",
    "        nn.Flatten(), nn.Dropout(dropout), nn.Linear(d_model * (l_win-1), 1)\n",
    "    )\n",
    "    model = TransformerModel(\n",
    "        TransformerEncoder(TransformerEncoderLayer(\n",
    "            d_model, c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(position),\n",
    "        final_linear,\n",
    "        conv\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_fnet_hybrid(N, d_model, l_win, device, kernel_size, d_ff=0, h=8, dropout=0.1):\n",
    "    if (d_ff == 0):\n",
    "        d_ff = d_model * 4\n",
    "    c = copy.deepcopy\n",
    "    conv = ConvLayer(d_model, kernel_size)\n",
    "    attn = MultiHeadAttention(h, d_model, device, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout, l_win)\n",
    "    final_linear = nn.Sequential(\n",
    "        nn.Flatten(), nn.Dropout(dropout), nn.Linear(d_model * (l_win-1), 1)\n",
    "    )\n",
    "    fft = FourierFFTLayer()\n",
    "    model = FNetHybridModel(\n",
    "        TransformerEncoder(TransformerEncoderLayer(\n",
    "            d_model, c(attn), c(ff), dropout), 1),\n",
    "        nn.Sequential(position),\n",
    "        final_linear,\n",
    "        FNetEncoder(FNetEncoderLayer(d_model, c(fft), c(ff), dropout), N - 1), \n",
    "        conv\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2df6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, config, mode):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        self.load_dataset(config)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            data = self.data[idx, :, :]\n",
    "            label = self.label[idx]\n",
    "            return data, label\n",
    "        else:\n",
    "            data = self.data[idx, :, :]\n",
    "            label = self.label[idx]\n",
    "            return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def load_dataset(self, config):\n",
    "        if self.mode == 'train':\n",
    "            train_df = pd.read_csv(\"/home/quanhhh/Documents/Improve-RUL-Prediction/preprocessed_data/train_003.csv\")\n",
    "\n",
    "            def gen_sequence(id_df, seq_length, seq_cols):\n",
    "                data_array = id_df[seq_cols].values\n",
    "                num_elements = data_array.shape[0]\n",
    "                for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "                    yield data_array[start:stop, :]\n",
    "            \n",
    "            \n",
    "#             #FD001\n",
    "#             sensor_cols = [\n",
    "#                 \"s2\",\n",
    "#                 \"s3\",\n",
    "#                 \"s4\",\n",
    "#                 \"s7\",\n",
    "#                 \"s8\",\n",
    "#                 \"s9\",\n",
    "#                 \"s11\",\n",
    "#                 \"s12\",\n",
    "#                 \"s13\",\n",
    "#                 \"s14\",\n",
    "#                 \"s15\",\n",
    "#                 \"s17\",\n",
    "#                 \"s20\",\n",
    "#                 \"s21\",\n",
    "#             ]\n",
    "#             sequence_cols = [\"setting1\", \"setting2\"]\n",
    "\n",
    "            # #FD002\n",
    "\n",
    "            # sensor_cols = [\n",
    "            #     \"s1\",\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s5\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s10\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s16\",\n",
    "            #     \"s17\",\n",
    "            #     \"s18\",\n",
    "            #     \"s19\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            \n",
    "            #FD003\n",
    "            sensor_cols = [\n",
    "                \"s2\",\n",
    "                \"s3\",\n",
    "                \"s4\",\n",
    "                \"s6\",\n",
    "                \"s7\",\n",
    "                \"s8\",\n",
    "                \"s9\",\n",
    "                \"s10\",\n",
    "                \"s11\",\n",
    "                \"s12\",\n",
    "                \"s13\",\n",
    "                \"s14\",\n",
    "                \"s15\",\n",
    "                \"s17\",\n",
    "                \"s20\",\n",
    "                \"s21\",\n",
    "            ]\n",
    "            sequence_cols = [\"setting1\", \"setting2\"]\n",
    "\n",
    "            # #FD004\n",
    "  \n",
    "            # sensor_cols = [\n",
    "            #     \"s1\",\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s5\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s10\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s16\",\n",
    "            #     \"s17\",\n",
    "            #     \"s18\",\n",
    "            #     \"s19\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            \n",
    "\n",
    "            sequence_cols.extend(sensor_cols)\n",
    "            # generator for the sequences\n",
    "            seq_gen = (list(gen_sequence(train_df[train_df['id']==id], self.config['l_win'], sequence_cols)) \n",
    "                      for id in train_df['id'].unique())\n",
    "\n",
    "            # generate sequences and convert to numpy array\n",
    "            seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "\n",
    "            # function to generate labels\n",
    "            def gen_labels(id_df, seq_length, label):\n",
    "                data_array = id_df[label].values\n",
    "                num_elements = data_array.shape[0]\n",
    "                return data_array[seq_length:num_elements, :]\n",
    "\n",
    "            # generate labels\n",
    "            label_gen = [gen_labels(train_df[train_df['id']==id], self.config['l_win'], ['RUL']) \n",
    "                        for id in train_df['id'].unique()]\n",
    "            label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "            self.data = seq_array\n",
    "            self.label = label_array\n",
    "\n",
    "        else:\n",
    "            test_df = pd.read_csv(\"/home/quanhhh/Documents/Improve-RUL-Prediction/preprocessed_data/test_003.csv\")\n",
    "            \n",
    "#              #FD001\n",
    "\n",
    "#             sensor_cols = [\n",
    "#                 \"s2\",\n",
    "#                 \"s3\",\n",
    "#                 \"s4\",\n",
    "#                 \"s7\",\n",
    "#                 \"s8\",\n",
    "#                 \"s9\",\n",
    "#                 \"s11\",\n",
    "#                 \"s12\",\n",
    "#                 \"s13\",\n",
    "#                 \"s14\",\n",
    "#                 \"s15\",\n",
    "#                 \"s17\",\n",
    "#                 \"s20\",\n",
    "#                 \"s21\",\n",
    "#             ]\n",
    "#             sequence_cols = [\"setting1\", \"setting2\"]\n",
    "\n",
    "            # #FD002\n",
    "\n",
    "            # sensor_cols = [\n",
    "            #     \"s1\",\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s5\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s10\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s16\",\n",
    "            #     \"s17\",\n",
    "            #     \"s18\",\n",
    "            #     \"s19\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            \n",
    "            #FD003\n",
    "\n",
    "            sensor_cols = [\n",
    "                \"s2\",\n",
    "                \"s3\",\n",
    "                \"s4\",\n",
    "                \"s6\",\n",
    "                \"s7\",\n",
    "                \"s8\",\n",
    "                \"s9\",\n",
    "                \"s10\",\n",
    "                \"s11\",\n",
    "                \"s12\",\n",
    "                \"s13\",\n",
    "                \"s14\",\n",
    "                \"s15\",\n",
    "                \"s17\",\n",
    "                \"s20\",\n",
    "                \"s21\",\n",
    "            ]\n",
    "            sequence_cols = [\"setting1\", \"setting2\"]\n",
    "\n",
    "            # #FD004\n",
    "\n",
    "            # sensor_cols = [\n",
    "            #     \"s1\",\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s5\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s10\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s16\",\n",
    "            #     \"s17\",\n",
    "            #     \"s18\",\n",
    "            #     \"s19\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            \n",
    "            sequence_cols.extend(sensor_cols)\n",
    "\n",
    "            seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-config['l_win']:] \n",
    "                                  for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= config['l_win']]\n",
    "\n",
    "            seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "\n",
    "\n",
    "            y_mask = [len(test_df[test_df['id']==id]) >= config['l_win'] for id in test_df['id'].unique()]\n",
    "\n",
    "            label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
    "            label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "\n",
    "            self.data = seq_array_test_last\n",
    "            self.label = label_array_test_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75add324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer():\n",
    "    def __init__(self, model, train_data, criterion, optimizer, device, config):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.train_loss_list = list()\n",
    "        self.min_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.best_optimizer = None\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        train_loss = 0.0\n",
    "        self.model.train()\n",
    "        for x, rul in self.train_data:\n",
    "            self.model.zero_grad()\n",
    "            out = self.model(x.to(self.device).float())\n",
    "#             print(out.shape)\n",
    "#             break\n",
    "            loss = torch.sqrt(self.criterion(out.float(), rul.to(self.device).float()))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss\n",
    "\n",
    "            \n",
    "\n",
    "        train_loss = train_loss / len(self.train_data)\n",
    "        self.train_loss_list.append(train_loss)\n",
    "\n",
    "        if train_loss < self.min_loss:\n",
    "            self.min_loss = train_loss\n",
    "            self.best_model = deepcopy(self.model.state_dict())\n",
    "            self.best_optimizer = deepcopy(self.optimizer.state_dict())\n",
    "            self.best_epoch_in_round = epoch\n",
    "\n",
    "    def train(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        for epoch in range(1, self.config['n_epochs'] + 1):\n",
    "            self.train_epoch(epoch)\n",
    "\n",
    "        self.config['train_loss_list'] = self.train_loss_list\n",
    "\n",
    "    def update_config(self):\n",
    "        return self.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9084062",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/quanhhh/Documents/Improve-RUL-Prediction/configs/test.yml') as f:\n",
    "  config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a2b6fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'd_model': 18,\n",
       " 'dff': 128,\n",
       " 'dropout': 0.1,\n",
       " 'experiment': '2stacks_1nhead_120lwin_0_001lr_128dff_128batch_40epcs_0_1dropout',\n",
       " 'kernel_size': 8,\n",
       " 'l_win': 120,\n",
       " 'l_win_max': 31,\n",
       " 'lr': 0.001,\n",
       " 'model': 1,\n",
       " 'n_epochs': 40,\n",
       " 'n_head': 1,\n",
       " 'num_layers': 2,\n",
       " 'num_workers': 4,\n",
       " 'weight_decay': 0.001}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280777fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e302f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = TimeSeriesDataset(config, mode='train')\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=config['batch_size'],\n",
    "                          shuffle=True,\n",
    "                          num_workers=config['num_workers'])\n",
    "\n",
    "# 1 denotes Transformer and 2 denotes hybrid model\n",
    "if config['model'] == 1:\n",
    "    model = create_transformer(N=config['num_layers'],\n",
    "                                     d_model=config['d_model'],\n",
    "                                     l_win=config['l_win'],\n",
    "                                     device=None,\n",
    "                                     kernel_size=config['kernel_size'],\n",
    "                                     d_ff=config['dff'],\n",
    "                                     h=config['n_head'],\n",
    "                                     dropout=config['dropout'])\n",
    "elif config['model'] == 2:\n",
    "    model = create_fnet_hybrid(N=config['num_layers'],\n",
    "                                     d_model=config['d_model'],\n",
    "                                     l_win=config['l_win'],\n",
    "                                     device=None,\n",
    "                                     kernel_size=config['kernel_size'],\n",
    "                                     d_ff=config['dff'],\n",
    "                                     h=config['n_head'],\n",
    "                                     dropout=config['dropout'])\n",
    "\n",
    "model.to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config['weight_decay'])\n",
    "criterion = nn.MSELoss()\n",
    "trainer = ModelTrainer(model, train_loader, criterion, optimizer, device, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e842bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_epoch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89105787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4a9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f366907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab2194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fcf9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f74dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc967ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c74df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc2ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dff1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc43ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5da90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26f653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
