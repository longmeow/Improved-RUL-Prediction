{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7XMhKM3hEZCZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlongmeow\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "otiUHA1UvLUp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.normalization import LayerNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JpGye-k7w0s_"
   },
   "outputs": [],
   "source": [
    "with open('/home/quanhhh/Documents/Improve-RUL-Prediction/bo.yml') as f:\n",
    "  sweep_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S8pu_VZPvLe1"
   },
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"1-D Convolution layer to extract high-level features of each time-series input\n",
    "    :param n_features: Number of input features/nodes (d_model)\n",
    "    :param window_size: length of the input sequence\n",
    "    :param kernel_size: size of kernel to use in the convolution operation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, kernel_size):\n",
    "        super().__init__()\n",
    "        self.padding = nn.ConstantPad1d((kernel_size - 1) // 2, 0.0)\n",
    "        self.conv = nn.Conv1d(in_channels=n_features, out_channels=n_features, kernel_size=kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    " \n",
    "    def forward(self, src):\n",
    "        src = src.permute(0, 2, 1)\n",
    "        src = self.padding(src)\n",
    "        src = self.relu(self.conv(src))\n",
    "        return src.permute(0, 2, 1)  # Permute back\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, linear, conv):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed\n",
    "        self.linear = linear\n",
    "        self.conv = conv\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.conv(src)\n",
    "        output = F.relu(self.linear(\n",
    "            self.encoder(self.src_embed(src), src_mask)))\n",
    "        return output\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    Produce N identical layers\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Apply residual connection to any sublayer with the same size.\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder is made up of self-attn and feed foward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "def attention(query, key, value, device, mask=None, dropout=0.0):\n",
    "    \"\"\"Compute the Scaled Dot-Product Attention\"\"\"\n",
    "    query = query.to(device)\n",
    "    key = query.to(device)\n",
    "    value = query.to(device)\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.to(device)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    p_attn = F.dropout(p_attn, p=dropout)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, device, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Takes in model size and number of heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.p = dropout\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        x, self.attn = attention(query, key, value, self.device, mask=mask, dropout=self.p)\n",
    "        # 3) \"Concat\" using a view and apply a final linear\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Torch linears have a \"b\" by default.\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Implements the PE function. \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        try:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        except:\n",
    "            div_term = torch.exp(torch.arange(0, d_model - 1, 2)\n",
    "                                 * (-math.log(10000.0) / d_model))\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1), :], requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class FNetHybridModel(nn.Module):\n",
    "    def __init__(self, trans, src_embed, linear, fnet, conv):\n",
    "        super().__init__()\n",
    "        self.trans = trans\n",
    "        self.src_embed = src_embed\n",
    "        self.linear = linear\n",
    "        self.fnet = fnet\n",
    "        self.conv = conv \n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.conv(src)\n",
    "        output = F.relu(self.linear(self.fnet(\n",
    "            self.trans(self.src_embed(src), src_mask))))\n",
    "        return output\n",
    "\n",
    "\n",
    "class FNetEncoder(nn.Module):\n",
    "    \"Core Fnet is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Pass the input through each layer in turn\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "\n",
    "class FNetEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder is made up of a Fourier Mixing Layer and a FF Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, fft, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.fft = fft\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sublayer[0](x, lambda x: self.fft(x, x, x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class FourierFFTLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        assert query is key\n",
    "        assert key is value\n",
    "\n",
    "        x = query\n",
    "        return torch.real(torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2))\n",
    "\n",
    "\n",
    "def create_transformer(N , d_model, l_win, device, kernel_size, d_ff=0, h=8, dropout=0.1):\n",
    "    if (d_ff == 0):\n",
    "        d_ff = d_model * 4\n",
    "    c = copy.deepcopy\n",
    "    conv = ConvLayer(d_model, kernel_size)\n",
    "    attn = MultiHeadAttention(h, d_model, device, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout, l_win)\n",
    "    final_linear = nn.Sequential(\n",
    "        nn.Flatten(), nn.Dropout(dropout), nn.Linear(d_model * l_win, 1)\n",
    "    )\n",
    "    model = TransformerModel(\n",
    "        TransformerEncoder(TransformerEncoderLayer(\n",
    "            d_model, c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(position),\n",
    "        final_linear,\n",
    "        conv\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_fnet_hybrid(N, d_model, l_win, device, kernel_size, d_ff=0, h=8, dropout=0.1):\n",
    "    if (d_ff == 0):\n",
    "        d_ff = d_model * 4\n",
    "    c = copy.deepcopy\n",
    "    conv = ConvLayer(d_model, kernel_size)\n",
    "    attn = MultiHeadAttention(h, d_model, device, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout, l_win)\n",
    "    final_linear = nn.Sequential(\n",
    "        nn.Flatten(), nn.Dropout(dropout), nn.Linear(d_model * l_win, 1)\n",
    "    )\n",
    "    fft = FourierFFTLayer()\n",
    "    model = FNetHybridModel(\n",
    "        TransformerEncoder(TransformerEncoderLayer(\n",
    "            d_model, c(attn), c(ff), dropout), 1),\n",
    "        nn.Sequential(position),\n",
    "        final_linear,\n",
    "        FNetEncoder(FNetEncoderLayer(d_model, c(fft), c(ff), dropout), N - 1), \n",
    "        conv\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#dataloader.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, config, mode):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        self.load_dataset(config)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            data = self.data[idx, :, :]\n",
    "            label = self.label[idx]\n",
    "            return data, label\n",
    "        else:\n",
    "            data = self.data[idx, :, :]\n",
    "            label = self.label[idx]\n",
    "            return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def load_dataset(self, config):\n",
    "        if self.mode == 'train':\n",
    "            train_df = pd.read_csv(\"./preprocessed_data/train_004.csv\")\n",
    "\n",
    "            def gen_sequence(id_df, seq_length, seq_cols):\n",
    "                data_array = id_df[seq_cols].values\n",
    "                num_elements = data_array.shape[0]\n",
    "                for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "                    yield data_array[start:stop, :]\n",
    "            \n",
    "            \n",
    "            # #FD001\n",
    "            # sensor_cols = [\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s17\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\"]\n",
    "\n",
    "            # FD002\n",
    "\n",
    "            # sensor_cols = [\n",
    "            #     \"s1\",\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s5\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s10\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s16\",\n",
    "            #     \"s17\",\n",
    "            #     \"s18\",\n",
    "            #     \"s19\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            \n",
    "            # #FD003\n",
    "            # sensor_cols = [\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s6\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s10\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s17\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\"]\n",
    "\n",
    "            # #FD004\n",
    "  \n",
    "            sensor_cols = [\n",
    "                \"s1\",\n",
    "                \"s2\",\n",
    "                \"s3\",\n",
    "                \"s4\",\n",
    "                \"s5\",\n",
    "                \"s7\",\n",
    "                \"s8\",\n",
    "                \"s9\",\n",
    "                \"s10\",\n",
    "                \"s11\",\n",
    "                \"s12\",\n",
    "                \"s13\",\n",
    "                \"s14\",\n",
    "                \"s15\",\n",
    "                \"s16\",\n",
    "                \"s17\",\n",
    "                \"s18\",\n",
    "                \"s19\",\n",
    "                \"s20\",\n",
    "                \"s21\",\n",
    "            ]\n",
    "            sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            \n",
    "\n",
    "            sequence_cols.extend(sensor_cols)\n",
    "            # generator for the sequences\n",
    "            seq_gen = (list(gen_sequence(train_df[train_df['id']==id], self.config['l_win'], sequence_cols)) \n",
    "                      for id in train_df['id'].unique())\n",
    "\n",
    "            # generate sequences and convert to numpy array\n",
    "            seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "\n",
    "            # function to generate labels\n",
    "            def gen_labels(id_df, seq_length, label):\n",
    "                data_array = id_df[label].values\n",
    "                num_elements = data_array.shape[0]\n",
    "                return data_array[seq_length:num_elements, :]\n",
    "\n",
    "            # generate labels\n",
    "            label_gen = [gen_labels(train_df[train_df['id']==id], self.config['l_win'], ['RUL']) \n",
    "                        for id in train_df['id'].unique()]\n",
    "            label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "            self.data = seq_array\n",
    "            self.label = label_array\n",
    "\n",
    "        else:\n",
    "            test_df = pd.read_csv(\"./preprocessed_data/test_004.csv\")\n",
    "            \n",
    "            #  #FD001\n",
    "\n",
    "            # sensor_cols = [\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s17\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\"]\n",
    "\n",
    "            #FD002\n",
    "\n",
    "            # sensor_cols = [\n",
    "            #     \"s1\",\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s5\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s10\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s16\",\n",
    "            #     \"s17\",\n",
    "            #     \"s18\",\n",
    "            #     \"s19\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            \n",
    "            # # #FD003\n",
    "\n",
    "            # sensor_cols = [\n",
    "            #     \"s2\",\n",
    "            #     \"s3\",\n",
    "            #     \"s4\",\n",
    "            #     \"s6\",\n",
    "            #     \"s7\",\n",
    "            #     \"s8\",\n",
    "            #     \"s9\",\n",
    "            #     \"s10\",\n",
    "            #     \"s11\",\n",
    "            #     \"s12\",\n",
    "            #     \"s13\",\n",
    "            #     \"s14\",\n",
    "            #     \"s15\",\n",
    "            #     \"s17\",\n",
    "            #     \"s20\",\n",
    "            #     \"s21\",\n",
    "            # ]\n",
    "            # sequence_cols = [\"setting1\", \"setting2\"]\n",
    "\n",
    "            # #FD004\n",
    "\n",
    "            sensor_cols = [\n",
    "                \"s1\",\n",
    "                \"s2\",\n",
    "                \"s3\",\n",
    "                \"s4\",\n",
    "                \"s5\",\n",
    "                \"s7\",\n",
    "                \"s8\",\n",
    "                \"s9\",\n",
    "                \"s10\",\n",
    "                \"s11\",\n",
    "                \"s12\",\n",
    "                \"s13\",\n",
    "                \"s14\",\n",
    "                \"s15\",\n",
    "                \"s16\",\n",
    "                \"s17\",\n",
    "                \"s18\",\n",
    "                \"s19\",\n",
    "                \"s20\",\n",
    "                \"s21\",\n",
    "            ]\n",
    "            sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            \n",
    "            sequence_cols.extend(sensor_cols)\n",
    "\n",
    "            seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-config['l_win']:] \n",
    "                                  for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= config['l_win']]\n",
    "\n",
    "            seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "\n",
    "\n",
    "            y_mask = [len(test_df[test_df['id']==id]) >= config['l_win'] for id in test_df['id'].unique()]\n",
    "\n",
    "            label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
    "            label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "\n",
    "            self.data = seq_array_test_last\n",
    "            self.label = label_array_test_last\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#trainer.py\n",
    "class ModelTrainer():\n",
    "    def __init__(self, model, train_data, criterion, optimizer, device, config):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.train_loss_list = list()\n",
    "        self.min_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.best_optimizer = None\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        train_loss = 0.0\n",
    "        self.model.train()\n",
    "        for x, rul in self.train_data:\n",
    "            self.model.zero_grad()\n",
    "            out = self.model(x.to(self.device).float())\n",
    "            loss = torch.sqrt(self.criterion(out.float(), rul.to(self.device).float()))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss\n",
    "            \n",
    "\n",
    "        train_loss = train_loss / len(self.train_data)\n",
    "        wandb.log({\"train loss\": train_loss})\n",
    "        self.train_loss_list.append(train_loss)\n",
    "\n",
    "        if train_loss < self.min_loss:\n",
    "            self.min_loss = train_loss\n",
    "            self.best_model = deepcopy(self.model.state_dict())\n",
    "            self.best_optimizer = deepcopy(self.optimizer.state_dict())\n",
    "            self.best_epoch_in_round = epoch\n",
    "\n",
    "    def train(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        for epoch in range(1, self.config['n_epochs'] + 1):\n",
    "            self.train_epoch(epoch)\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "\n",
    "\n",
    "        self.config['train_loss_list'] = self.train_loss_list\n",
    "\n",
    "    def update_config(self):\n",
    "        return self.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fo9qmgHw0q6c"
   },
   "outputs": [],
   "source": [
    "#train.py\n",
    "def training():\n",
    "    with wandb.init(config=sweep_config):\n",
    "        config = wandb.config\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_data = TimeSeriesDataset(config, mode='train')\n",
    "        train_loader = DataLoader(train_data,\n",
    "                                  batch_size=config['batch_size'],\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=config['num_workers'])\n",
    "\n",
    "        if config['model'] == 1:\n",
    "            model = create_transformer(N=config['num_layers'],\n",
    "                                            d_model=config['d_model'],\n",
    "                                            l_win=config['l_win'],\n",
    "                                            device=None,\n",
    "                                            kernel_size=config['kernel_size'],\n",
    "                                            d_ff=config['dff'],\n",
    "                                            h=config['n_head'],\n",
    "                                            dropout=config['dropout'])\n",
    "        if config['model'] == 2:\n",
    "            model = create_fnet_hybrid(N=config['num_layers'],\n",
    "                                            d_model=config['d_model'],\n",
    "                                            l_win=config['l_win'],\n",
    "                                            device=None,\n",
    "                                            kernel_size=config['kernel_size'],\n",
    "                                            d_ff=config['dff'],\n",
    "                                            h=config['n_head'],\n",
    "                                            dropout=config['dropout'])\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config['weight_decay'])\n",
    "        criterion = nn.MSELoss()\n",
    "        trainer = ModelTrainer(model, train_loader, criterion, optimizer, device, config)\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        #inference.py\n",
    "\n",
    "        test_data = TimeSeriesDataset(config, mode='test')\n",
    "        test_loader = DataLoader(test_data,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=config['num_workers'])\n",
    "\n",
    "        model.to(device)\n",
    "        test_loss = 0.0\n",
    "        criterion = nn.MSELoss()\n",
    "        test_loss_list = list()\n",
    "        pred_list = list()\n",
    "        with torch.no_grad():\n",
    "            for x, rul in test_loader:\n",
    "                out = model(x.to(device).float())\n",
    "                loss = torch.sqrt(criterion(out.float(), rul.to(device).float()))\n",
    "                test_loss += loss\n",
    "                test_loss_list.append(loss)\n",
    "                pred_list.append(out.float())\n",
    "\n",
    "        test_loss_avg = test_loss / len(test_loader)\n",
    "        truth_list = [rul.float().item() for x, rul in test_loader]\n",
    "        training_time = (time.perf_counter() - start) / 60\n",
    "        config['truth_list'] = truth_list\n",
    "        config['pred_list'] = pred_list\n",
    "        config['test_loss_avg'] = test_loss_avg\n",
    "        config['test_loss_list_per_id'] = test_loss_list\n",
    "        config['training_time'] = training_time\n",
    "        wandb.log({\"test_loss_avg\": test_loss_avg})\n",
    "        wandb.log({\"training_time\": training_time})\n",
    "        if test_loss_avg < 12.4:\n",
    "            wandb.alert(\n",
    "                title='longmeow',\n",
    "                text=f'test_loss_avg {test_loss_avg} is below the theshold 12.4',\n",
    "            )\n",
    "            print('Alert triggered')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. weight_decay uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 2nev6ie8\n",
      "Sweep URL: https://wandb.ai/longmeow/improve-rul-prediction/sweeps/2nev6ie8\n"
     ]
    }
   ],
   "source": [
    "# sweep_id = wandb.sweep(sweep=sweep_config, project='improve-rul-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XywYZZplSBha"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  5 10:07:58 AM +07 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4t0dyzvz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \td_model: 23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdff: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1481999058765633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl_win: 122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.003997124489555377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 89\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.006727809613743149\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/quanhhh/Documents/Improve-RUL-Prediction/wandb/run-20221205_100801-4t0dyzvz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/longmeow/improve-rul-prediction/runs/4t0dyzvz\" target=\"_blank\">skilled-sweep-1</a></strong> to <a href=\"https://wandb.ai/longmeow/improve-rul-prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/longmeow/improve-rul-prediction/sweeps/2nev6ie8\" target=\"_blank\">https://wandb.ai/longmeow/improve-rul-prediction/sweeps/2nev6ie8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!date\n",
    "wandb.agent('improve-rul-prediction/2nev6ie8', training, count=100000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e41069438920b677da0f5961bc4e6feb5d97eaf860276a7a70f8fcc8c0e79bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
